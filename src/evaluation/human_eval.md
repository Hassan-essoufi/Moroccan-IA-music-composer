# Human Evaluation Protocol for Moroccan Music Transformer

## 1. Introduction

This document outlines the **human evaluation methodology** for assessing the quality and coherence of music generated by the Moroccan Music Transformer model.  
The evaluation is designed to provide **quantitative and qualitative measures** of musical coherence, comparing generated compositions against their original references.

---

## 2. Evaluation Objectives

The primary objectives of this evaluation are:

1. **Assess musical coherence**: Evaluate the logical flow, melody, harmony, and rhythm of generated music.
2. **Compare with original compositions**: Measure how closely generated pieces align with reference MIDI files.
3. **Complement automatic metrics**: Provide a human-centered validation alongside computational similarity scores.

---

## 3. Evaluation Metrics

### 3.1 Human Coherence Score (5-point Likert scale)

Annotators are asked to rate each generated music piece based on **musical coherence**, using the following scale:

| Score | Description |
|-------|-------------|
| 1     | Very incoherent: Melody, rhythm, and harmony are mostly disrupted; music is difficult to follow. |
| 2     | Incoherent: Noticeable musical inconsistencies affecting the listening experience. |
| 3     | Moderately coherent: Some errors or deviations present, but overall structure is recognizable. |
| 4     | Coherent: Minor inconsistencies; music is largely faithful to the original. |
| 5     | Very coherent: Melody, harmony, and rhythm closely follow the original composition; musically consistent. |

> **Guidelines for Annotators**:  
> - Focus on **melody, rhythm, harmonic progression, and structural integrity**.  
> - Ignore audio rendering quality (instrument timbre or production artifacts).  
> - Multiple listens are encouraged for accurate scoring.

### 3.2 Automatic Audio Similarity Score

To complement human evaluation:

- Generated and reference MIDI files are converted to **WAV audio** using **FluidSynth**.  
- **Mel spectrograms** are computed for each audio file.  
- **Cosine similarity** between flattened spectrograms provides a quantitative similarity score:  
  - **1.0** → highly similar  
  - **0.0** → highly dissimilar  

This score serves as a reference and helps identify cases where human attention should focus.

---

## 4. Evaluation Procedure

1. **Sample Selection**: Choose a representative subset of generated MIDI files from the dataset.  
2. **Conversion to Audio**: Convert both generated and reference MIDI files to WAV using FluidSynth.  
3. **Optional Pre-Filtering**: Compute automatic similarity scores to detect highly divergent pieces.  
4. **Human Annotation**:  
   - Present the generated audio (and optionally the reference) to annotators in a **blind evaluation setup**.  
   - Each annotator assigns a **coherence score (1–5)**.  
   - Annotators may provide **qualitative comments** highlighting errors or musical deviations.  
5. **Data Aggregation**:  
   - Compute **mean**, **standard deviation**, and **distribution** of human scores.  
   - Optional: correlate human scores with automatic similarity scores to analyze model performance.

---

## 5. Annotation Template

| Sample ID | Generated MIDI | Reference MIDI | Human Coherence Score (1-5) | Annotator Notes |
|-----------|----------------|----------------|-----------------------------|----------------|
| 1         | gnawa_gen_001.mid | gnawa_orig_001.mid |                         |                |
| 2         | gnawa_gen_002.mid | gnawa_orig_002.mid |                         |                |
| 3         | gnawa_gen_003.mid | gnawa_orig_003.mid |                         |                |
| ...       | ...            | ...            |                             |                |

> **Instructions**: Annotators fill in the score and add comments if needed to document deviations or errors.

---

## 6. Reporting and Analysis

- **Quantitative Analysis**:  
  - Compute **mean score** per dataset split (train/val).  
  - Analyze **variance** to assess inter-annotator agreement.  
- **Qualitative Analysis**:  
  - Review annotator comments to identify common errors (rhythm, melody, harmony).  
- **Comparison with Automatic Metrics**:  
  - Correlate human scores with cosine similarity to validate the reliability of the automatic metric.  

---

## 7. References

- *PrettyMIDI: A Python Library for Handling MIDI Data*. [GitHub](https://github.com/craffel/pretty-midi)  
- *librosa: Audio and Music Signal Analysis in Python*. [Librosa](https://librosa.org/)  

